{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\"\"\"\n",
    "changed from https://www.kaggle.com/anycode/simple-nn-baseline\n",
    "\n",
    "to run this kernel, pip install ultimate first from your custom packages\n",
    "\"\"\"\n",
    "import gc, sys\n",
    "gc.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering(is_train=True,debug=True):\n",
    "    test_idx = None\n",
    "    if is_train: \n",
    "        print(\"processing train.csv\")\n",
    "        if debug == True:\n",
    "            df = pd.read_csv('input/train_V2.csv', nrows=10000)\n",
    "        else:\n",
    "            df = pd.read_csv('input/train_V2.csv')           \n",
    "\n",
    "        df = df[df['maxPlace'] > 1]\n",
    "    else:\n",
    "        print(\"processing test.csv\")\n",
    "        df = pd.read_csv('input/test_V2.csv')\n",
    "        test_idx = df.Id\n",
    "    \n",
    "    print(\"remove some columns\")\n",
    "    target = 'winPlacePerc'\n",
    "\n",
    "    print(\"Adding Features\")\n",
    " \n",
    "    df['headshotrate'] = df['kills']/df['headshotKills']\n",
    "    df['killStreakrate'] = df['killStreaks']/df['kills']\n",
    "    df['healthitems'] = df['heals'] + df['boosts']\n",
    "    df['totalDistance'] = df['rideDistance'] + df[\"walkDistance\"] + df[\"swimDistance\"]\n",
    "    df['killPlace_over_maxPlace'] = df['killPlace'] / df['maxPlace']\n",
    "    df['headshotKills_over_kills'] = df['headshotKills'] / df['kills']\n",
    "    df['distance_over_weapons'] = df['totalDistance'] / df['weaponsAcquired']\n",
    "    df['walkDistance_over_heals'] = df['walkDistance'] / df['heals']\n",
    "    df['walkDistance_over_kills'] = df['walkDistance'] / df['kills']\n",
    "    df['killsPerWalkDistance'] = df['kills'] / df['walkDistance']\n",
    "    df[\"skill\"] = df[\"headshotKills\"] + df[\"roadKills\"]\n",
    "\n",
    "    df[df == np.Inf] = np.NaN\n",
    "    df[df == np.NINF] = np.NaN\n",
    "    \n",
    "    print(\"Removing Na's From DF\")\n",
    "    df.fillna(0, inplace=True)\n",
    "\n",
    "    \n",
    "    features = list(df.columns)\n",
    "    features.remove(\"Id\")\n",
    "    features.remove(\"matchId\")\n",
    "    features.remove(\"groupId\")\n",
    "    features.remove(\"matchType\") \n",
    "    \n",
    "    y = None\n",
    "    \n",
    "    if is_train: \n",
    "        print(\"get target\")\n",
    "        y = np.array(df.groupby(['matchId','groupId'])[target].agg('mean'), dtype=np.float64)\n",
    "        features.remove(target)\n",
    "\n",
    "    print(\"get group mean feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('mean')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    \n",
    "    if is_train: df_out = agg.reset_index()[['matchId','groupId']]\n",
    "    else: df_out = df[['matchId','groupId']]\n",
    "\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_mean\", \"_mean_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"get group max feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('max')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_max\", \"_max_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"get group min feature\")\n",
    "    agg = df.groupby(['matchId','groupId'])[features].agg('min')\n",
    "    agg_rank = agg.groupby('matchId')[features].rank(pct=True).reset_index()\n",
    "    df_out = df_out.merge(agg.reset_index(), suffixes=[\"\", \"\"], how='left', on=['matchId', 'groupId'])\n",
    "    df_out = df_out.merge(agg_rank, suffixes=[\"_min\", \"_min_rank\"], how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"get group size feature\")\n",
    "    agg = df.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId', 'groupId'])\n",
    "    \n",
    "    print(\"get match mean feature\")\n",
    "    agg = df.groupby(['matchId'])[features].agg('mean').reset_index()\n",
    "    df_out = df_out.merge(agg, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\n",
    "    \n",
    "    print(\"get match size feature\")\n",
    "    agg = df.groupby(['matchId']).size().reset_index(name='match_size')\n",
    "    df_out = df_out.merge(agg, how='left', on=['matchId'])\n",
    "    \n",
    "    df_out.drop([\"matchId\", \"groupId\"], axis=1, inplace=True)\n",
    "\n",
    "    X = df_out\n",
    "    \n",
    "    feature_names = list(df_out.columns)\n",
    "\n",
    "    del df, df_out, agg, agg_rank\n",
    "    gc.collect()\n",
    "\n",
    "    return X, y, feature_names, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing train.csv\n",
      "remove some columns\n",
      "Adding Features\n",
      "Removing Na's From DF\n",
      "get target\n",
      "get group mean feature\n",
      "get group max feature\n",
      "get group min feature\n",
      "get group size feature\n",
      "get match mean feature\n",
      "get match size feature\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, train_columns, _ = feature_engineering(True,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing test.csv\n",
      "remove some columns\n",
      "Adding Features\n",
      "Removing Na's From DF\n",
      "get group mean feature\n",
      "get group max feature\n",
      "get group min feature\n",
      "get group size feature\n",
      "get match mean feature\n",
      "get match size feature\n"
     ]
    }
   ],
   "source": [
    "x_test, _, _ , test_idx = feature_engineering(False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thanks and credited to https://www.kaggle.com/gemartin who created this wonderful mem reducer\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() \n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() \n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 4021060096.00 MB\n",
      "Memory usage after optimization is: 948516192.00 MB\n",
      "Decreased by 76.4%\n",
      "Memory usage of dataframe is 3837401216.00 MB\n",
      "Memory usage after optimization is: 903259258.00 MB\n",
      "Decreased by 76.5%\n"
     ]
    }
   ],
   "source": [
    "x_train = reduce_mem_usage(x_train)\n",
    "x_test = reduce_mem_usage(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benson\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:111: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n",
      "C:\\Users\\Benson\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:116: UserWarning: Found `early_stopping_rounds` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[1000]\ttraining's l1: 0.0282507\tvalid_1's l1: 0.0288693\n",
      "[2000]\ttraining's l1: 0.0270028\tvalid_1's l1: 0.0281773\n",
      "[3000]\ttraining's l1: 0.026192\tvalid_1's l1: 0.0278832\n",
      "[4000]\ttraining's l1: 0.0255316\tvalid_1's l1: 0.0277071\n",
      "[5000]\ttraining's l1: 0.0249701\tvalid_1's l1: 0.0275958\n",
      "[6000]\ttraining's l1: 0.0244602\tvalid_1's l1: 0.0275161\n",
      "[7000]\ttraining's l1: 0.0240017\tvalid_1's l1: 0.0274589\n",
      "[8000]\ttraining's l1: 0.0235657\tvalid_1's l1: 0.0274077\n",
      "[9000]\ttraining's l1: 0.0231617\tvalid_1's l1: 0.0273648\n",
      "[10000]\ttraining's l1: 0.0227831\tvalid_1's l1: 0.0273305\n",
      "[11000]\ttraining's l1: 0.022418\tvalid_1's l1: 0.0273009\n",
      "[12000]\ttraining's l1: 0.0220675\tvalid_1's l1: 0.0272781\n",
      "[13000]\ttraining's l1: 0.0217335\tvalid_1's l1: 0.0272615\n",
      "[14000]\ttraining's l1: 0.021404\tvalid_1's l1: 0.0272413\n",
      "[15000]\ttraining's l1: 0.0210929\tvalid_1's l1: 0.0272243\n",
      "[16000]\ttraining's l1: 0.0207937\tvalid_1's l1: 0.0272085\n",
      "[17000]\ttraining's l1: 0.0205064\tvalid_1's l1: 0.0271988\n",
      "[18000]\ttraining's l1: 0.0202239\tvalid_1's l1: 0.0271857\n",
      "[19000]\ttraining's l1: 0.019947\tvalid_1's l1: 0.0271732\n",
      "[20000]\ttraining's l1: 0.0196855\tvalid_1's l1: 0.0271666\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[20000]\ttraining's l1: 0.0196855\tvalid_1's l1: 0.0271666\n"
     ]
    }
   ],
   "source": [
    "#excluded_features = []\n",
    "#use_cols = [col for col in df_train.columns if col not in excluded_features]\n",
    "\n",
    "train_index = round(int(x_train.shape[0]*0.8))\n",
    "dev_X = x_train[:train_index] \n",
    "val_X = x_train[train_index:]\n",
    "dev_y = y_train[:train_index] \n",
    "val_y = y_train[train_index:] \n",
    "gc.collect();\n",
    "\n",
    "# custom function to run light gbm model\n",
    "def run_lgb(train_X, train_y, val_X, val_y, x_test):\n",
    "    params = {\"objective\" : \"regression\", \"metric\" : \"mae\", 'n_estimators':20000, 'early_stopping_rounds':200,\n",
    "              \"num_leaves\" : 31, \"learning_rate\" : 0.05, \"bagging_fraction\" : 0.7,\n",
    "               \"bagging_seed\" : 0, \"num_threads\" : 4,\"colsample_bytree\" : 0.7\n",
    "             }\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    lgval = lgb.Dataset(val_X, label=val_y)\n",
    "    model = lgb.train(params, lgtrain, valid_sets=[lgtrain, lgval], early_stopping_rounds=200, verbose_eval=1000)\n",
    "    \n",
    "    pred_test_y = model.predict(x_test, num_iteration=model.best_iteration)\n",
    "    return pred_test_y, model\n",
    "\n",
    "# Training the model #\n",
    "pred_test, model = run_lgb(dev_X, dev_y, val_X, val_y, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.read_csv(\"input/sample_submission_V2.csv\")\n",
    "df_test = pd.read_csv(\"input/test_V2.csv\")\n",
    "df_sub['winPlacePerc'] = pred_test\n",
    "# Restore some columns\n",
    "df_sub = df_sub.merge(df_test[[\"Id\", \"matchId\", \"groupId\", \"maxPlace\", \"numGroups\"]], on=\"Id\", how=\"left\")\n",
    "\n",
    "# Sort, rank, and assign adjusted ratio\n",
    "df_sub_group = df_sub.groupby([\"matchId\", \"groupId\"]).first().reset_index()\n",
    "df_sub_group[\"rank\"] = df_sub_group.groupby([\"matchId\"])[\"winPlacePerc\"].rank()\n",
    "df_sub_group = df_sub_group.merge(\n",
    "    df_sub_group.groupby(\"matchId\")[\"rank\"].max().to_frame(\"max_rank\").reset_index(), \n",
    "    on=\"matchId\", how=\"left\")\n",
    "df_sub_group[\"adjusted_perc\"] = (df_sub_group[\"rank\"] - 1) / (df_sub_group[\"numGroups\"] - 1)\n",
    "\n",
    "df_sub = df_sub.merge(df_sub_group[[\"adjusted_perc\", \"matchId\", \"groupId\"]], on=[\"matchId\", \"groupId\"], how=\"left\")\n",
    "df_sub[\"winPlacePerc\"] = df_sub[\"adjusted_perc\"]\n",
    "\n",
    "# Deal with edge cases\n",
    "df_sub.loc[df_sub.maxPlace == 0, \"winPlacePerc\"] = 0\n",
    "df_sub.loc[df_sub.maxPlace == 1, \"winPlacePerc\"] = 1\n",
    "\n",
    "# Align with maxPlace\n",
    "# Credit: https://www.kaggle.com/anycode/simple-nn-baseline-4\n",
    "subset = df_sub.loc[df_sub.maxPlace > 1]\n",
    "gap = 1.0 / (subset.maxPlace.values - 1)\n",
    "new_perc = np.around(subset.winPlacePerc.values / gap) * gap\n",
    "df_sub.loc[df_sub.maxPlace > 1, \"winPlacePerc\"] = new_perc\n",
    "\n",
    "# Edge case\n",
    "df_sub.loc[(df_sub.maxPlace > 1) & (df_sub.numGroups == 1), \"winPlacePerc\"] = 0\n",
    "assert df_sub[\"winPlacePerc\"].isnull().sum() == 0\n",
    "\n",
    "df_sub[[\"Id\", \"winPlacePerc\"]].to_csv(\"submission_adjusted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x2ae02380f98>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_model('lightgbm_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bst = lgb.Booster(model_file='lightgbm_model.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
